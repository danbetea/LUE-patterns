{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f518be28-5111-424f-8e34-f4a468733d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Author: Dan Betea                                                     #\n",
    "# (C) December 2022                                                     #\n",
    "# License: CC BY-SA 4.0                                                 #\n",
    "# License description: https://creativecommons.org/licenses/by-sa/4.0/  #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf15060a-88a7-4cb2-b57d-08d56e1c35af",
   "metadata": {},
   "source": [
    "# LUE patterns 2: linear regression with naive train/test split\n",
    "\n",
    "Consider the Laguerre-$\\alpha$ Unitary Ensemble (LUE-$\\alpha$) distribution on ordered tuples of $N$ positive real numbers $(\\lambda_1 < \\dots < \\lambda_N).$ That is, consider the probability measure\n",
    "\n",
    "$$P(\\lambda_1, \\dots, \\lambda_N)d \\lambda_1 \\dots d \\lambda_N \\propto \\prod_{1 \\leq i < j \\leq N} (\\lambda_i - \\lambda_j)^2 \\prod_{1 \\leq i \\leq N} \\lambda_i^{\\alpha-1} e^{-\\lambda_i} d \\lambda_i$$\n",
    "\n",
    "where $\\alpha > 0$ (notice the somewhat less standard $\\alpha$ convention we use for LUE). See [this Wikipedia article](https://en.wikipedia.org/wiki/Complex_Wishart_distribution) for the motivation behind this distribution and how it comes about when studying covariance matrices (see in particular the Eigenvalues section).\n",
    "\n",
    "We do linear regression (in Julia) on \n",
    "\n",
    "$$(\\log i, \\log E \\lambda_i^s)$$\n",
    "\n",
    "for $i$ in a certain interval like $1, \\dots, 10; 1, \\dots, \\log N$ or more generally $m_0, \\dots, m_0 + M - 1$. Here $\\lambda_i$ the $i$-th lowest eigenvalue of the LUE-$\\alpha$ ensemble and $s$ is a real number, taken negative for convergence. \n",
    "\n",
    "We consider several cases:\n",
    "\n",
    "- $m_0 = 1, M = 15$, $\\alpha = 4, s = -2$, $N \\in \\{1000, 5000, 10000\\}$, and 1000 total samples\n",
    "\n",
    "and we additionally consider two cases for splitting the data into training and testing: \n",
    "\n",
    "- first we try 60%-40%, and\n",
    "- then we try 80%-20%.\n",
    "\n",
    "One we have variables $y_i^{train}$ and $y_i^{test}$, computing the $R^2$ coefficient (metric) becomes a matter of choice. We compute three such coefficients: on the training set only, on the test set only, and out-of-sample on the train and test set as follows:\n",
    "\n",
    "$$R^2_{train} = 1 - \\frac{\\sum_i (y_i^{train} - \\hat{y}_i^{train})^2}{\\sum_i (y_i^{train} - \\bar{y}^{train})^2}, \\qquad\n",
    "R^2_{test} = 1 - \\frac{\\sum_i (y_i^{test} - \\hat{y}_i^{test})^2}{\\sum_i (y_i^{test} - \\bar{y}^{test})^2}, \\qquad\n",
    "R^2_{oos} = 1 - \\frac{\\sum_i (y_i^{test} - \\hat{y}_i^{test})^2}{\\sum_i (y_i^{test} - \\bar{y}^{train})^2}$$\n",
    "\n",
    "and the reason for $R^2_{oos}$ is the following: we check the test set performance against the benchmark *non-prediction model* we see on the training/fitted set, which is the model which outputs the mean of the training set $\\bar{y}^{train}$ no matter the data.\n",
    "\n",
    "**Important remark:** The dependent variable $x = (\\log i)_{m_0 \\leq i \\leq m_0+M}$ is deterministic, so $\\hat{y}^{train} = \\hat{y}^{test}$ using our setup.\n",
    "\n",
    "**Some other remarks:**\n",
    "\n",
    "- array indexing starts at 1 by default in Julia\n",
    "- the data files, containing iid samples $(\\lambda_1 < \\dots < \\lambda_N)$, are assumed to be in the same directory as the notebook, and be in the correct format\n",
    "- steps below can be automated; for this exploratory notebook they are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "403a742e-e169-4276-b483-85954eba0d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some packages we need\n",
    "using DataFrames          # for dataframes, surely not needed\n",
    "using GLM                 # for linear regression\n",
    "using DelimitedFiles      # for reading a big matrix out of a file easily\n",
    "using Plots               # for plotting \n",
    "using LaTeXStrings        # for LaTeX symbols inside plots\n",
    "using Statistics          # for the obvious reason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac24ff-d56b-45d4-accb-36ab6163a931",
   "metadata": {},
   "source": [
    "### First case: 60-40 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9711b38-a4f7-47f7-a7e8-abd200da8494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n",
       "\n",
       "Y ~ 1 + X\n",
       "\n",
       "Coefficients:\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "                Coef.  Std. Error        t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)  10.6401    0.0328259   324.14    <1e-26   10.5692    10.711\n",
       "X            -3.55653   0.0163515  -217.51    <1e-23   -3.59186   -3.52121\n",
       "──────────────────────────────────────────────────────────────────────────"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R squared on the training set is: 0.9997252840928547\n",
      "\n",
      "R squared (naive) on the test set is: 0.9996010133806793\n",
      "\n",
      "R squared (out-of-sample) on the test set is: 0.9996010244247339\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "N = 5000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n",
       "\n",
       "Y ~ 1 + X\n",
       "\n",
       "Coefficients:\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "                Coef.  Std. Error        t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)  13.7803    0.0365889   376.63    <1e-26   13.7013    13.8594\n",
       "X            -3.52498   0.0182259  -193.40    <1e-23   -3.56436   -3.48561\n",
       "──────────────────────────────────────────────────────────────────────────"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R squared on the training set is: 0.9996525781321135\n",
      "\n",
      "R squared (naive) on the test set is: 0.9995437008969466\n",
      "\n",
      "R squared (out-of-sample) on the test set is: 0.9995437012847317\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "N = 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n",
       "\n",
       "Y ~ 1 + X\n",
       "\n",
       "Coefficients:\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "                Coef.  Std. Error        t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)  15.2411    0.0304547   500.45    <1e-28   15.1753    15.3069\n",
       "X            -3.56084   0.0151703  -234.72    <1e-24   -3.59361   -3.52806\n",
       "──────────────────────────────────────────────────────────────────────────"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R squared on the training set is: 0.9997641013695571\n",
      "\n",
      "R squared (naive) on the test set is: 0.9986867445554634\n",
      "\n",
      "R squared (out-of-sample) on the test set is: 0.9986867990895594\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha_str = \"4.00\"\n",
    "alpha = parse(Float64, alpha_str)            # alpha-1 is the Laguerre parameter, alpha > 0\n",
    "Ns = [1000, 5000, 10000]                     # matrix sizes\n",
    "num_samples = 1000                           # number of samples, each sample is a line\n",
    "range_train = 1:600                          # range (samples to include) for training\n",
    "range_test = 601:1000                        # range (samples to include) for testing\n",
    "s = -2.0\n",
    "\n",
    "m0 = 1                                       # where to start with the linear regression\n",
    "M = 15                                       # we stop at m0 + M\n",
    "\n",
    "range_ms = m0 : m0 + M - 1                   # the range of data points used\n",
    "log_ms = [log(m) for m in range_ms]          # the x variable for regression\n",
    "\n",
    "reg_dict = Dict()\n",
    "coeff_dict = Dict()\n",
    "\n",
    "for N in Ns\n",
    "    println(\"N = $(N)\\n\")\n",
    "    A = readdlm(string(\"LUE_N_\", N,\"_alpha_\", alpha_str, \".txt\"), '\\t', Float64, '\\n')[1:num_samples, range_ms]\n",
    "    A_train = A[range_train, :]\n",
    "    A_test = A[range_test, :]\n",
    "    log_expectations_train = [log(mean(A_train[:, m - m0 + 1] .^ s)) for m in range_ms]\n",
    "    log_expectations_test = [log(mean(A_test[:, m - m0 + 1] .^ s)) for m in range_ms]\n",
    "    \n",
    "    data = DataFrame(X=log_ms, Y=log_expectations_train)\n",
    "    reg = lm(@formula(Y ~ X), data)\n",
    "    reg_dict[string(N)] = reg\n",
    "\n",
    "    display(reg)\n",
    "    \n",
    "    println()\n",
    "    println(\"R squared on the training set is: \", r2(reg))\n",
    "    println()\n",
    "    \n",
    "    # compute R^2 on the test set (see e.g. Wikipedia article), naively\n",
    "    SS_tot_test = sum((log_expectations_test .- mean(log_expectations_test)).^2)\n",
    "    SS_res_test = sum((log_expectations_test .- (coef(reg)[1] .+ coef(reg)[2] .* log_ms)).^2)\n",
    "    r2_test = 1 - SS_res_test/SS_tot_test\n",
    "    \n",
    "    println(\"R squared (naive) on the test set is: \", r2_test)\n",
    "    println()\n",
    "    \n",
    "    # compute out-of-sample OOS R^2, against predicted y_bar on the training set ! on the test set (see e.g. Wikipedia article), naively\n",
    "    SS_tot_train_test = sum((log_expectations_test .- mean(log_expectations_train)).^2)\n",
    "    r2_oos = 1 - SS_res_test/SS_tot_train_test\n",
    "    \n",
    "    println(\"R squared (out-of-sample) on the test set is: \", r2_oos)\n",
    "    println(\"\\n--------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    coeff_dict[string(N)] = Dict([(\"b0\", coef(reg)[1]), (\"b1\", coef(reg)[2]), \n",
    "            (\"r2_train\", r2(reg)), (\"r2_test\", r2_test), (\"r2_oos\", r2_oos)])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59456dfe-8fd1-4193-b58a-f6b8bd22ba16",
   "metadata": {},
   "source": [
    "### Second case: 60-40 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c53eb0d7-2bd6-4a3e-a23b-976bd72aabcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n",
       "\n",
       "Y ~ 1 + X\n",
       "\n",
       "Coefficients:\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "                Coef.  Std. Error        t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)  10.6371    0.0326739   325.55    <1e-26   10.5665    10.7077\n",
       "X            -3.55699   0.0162758  -218.55    <1e-23   -3.59215   -3.52182\n",
       "──────────────────────────────────────────────────────────────────────────"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R squared on the training set is: 0.9997278908057867\n",
      "\n",
      "R squared (naive) on the test set is: 0.9993046075393165\n",
      "\n",
      "R squared (out-of-sample) on the test set is: 0.9993046167559674\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "N = 5000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n",
       "\n",
       "Y ~ 1 + X\n",
       "\n",
       "Coefficients:\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "                Coef.  Std. Error        t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)  13.7651    0.0388747   354.09    <1e-26   13.6812    13.8491\n",
       "X            -3.51846   0.0193645  -181.70    <1e-22   -3.56029   -3.47662\n",
       "──────────────────────────────────────────────────────────────────────────"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R squared on the training set is: 0.9996063762935677\n",
      "\n",
      "R squared (naive) on the test set is: 0.9996495122867071\n",
      "\n",
      "R squared (out-of-sample) on the test set is: 0.9996495175602309\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "\n",
      "N = 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n",
       "\n",
       "Y ~ 1 + X\n",
       "\n",
       "Coefficients:\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "                Coef.  Std. Error        t  Pr(>|t|)  Lower 95%  Upper 95%\n",
       "──────────────────────────────────────────────────────────────────────────\n",
       "(Intercept)  15.1681    0.0373472   406.14    <1e-27   15.0874    15.2488\n",
       "X            -3.52533   0.0186036  -189.50    <1e-23   -3.56552   -3.48514\n",
       "──────────────────────────────────────────────────────────────────────────"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R squared on the training set is: 0.9996381045595585\n",
      "\n",
      "R squared (naive) on the test set is: 0.9996484001983278\n",
      "\n",
      "R squared (out-of-sample) on the test set is: 0.9996484010579837\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha_str = \"4.00\"\n",
    "alpha = parse(Float64, alpha_str)            # alpha-1 is the Laguerre parameter, alpha > 0\n",
    "Ns = [1000, 5000, 10000]                     # matrix sizes\n",
    "num_samples = 1000                           # number of samples, each sample is a line\n",
    "range_train = 201:1000                       # range (samples to include) for training\n",
    "range_test = 1:200                           # range (samples to include) for testing\n",
    "s = -2.0\n",
    "\n",
    "m0 = 1                                       # where to start with the linear regression\n",
    "M = 15                                       # we stop at m0 + M\n",
    "\n",
    "range_ms = m0 : m0 + M - 1                   # the range of data points used\n",
    "log_ms = [log(m) for m in range_ms]          # the x variable for regression\n",
    "\n",
    "reg_dict_2 = Dict()\n",
    "coeff_dict_2 = Dict()\n",
    "\n",
    "for N in Ns\n",
    "    println(\"N = $(N)\\n\")\n",
    "    A = readdlm(string(\"LUE_N_\", N,\"_alpha_\", alpha_str, \".txt\"), '\\t', Float64, '\\n')[1:num_samples, range_ms]\n",
    "    A_train = A[range_train, :]\n",
    "    A_test = A[range_test, :]\n",
    "    log_expectations_train = [log(mean(A_train[:, m - m0 + 1] .^ s)) for m in range_ms]\n",
    "    log_expectations_test = [log(mean(A_test[:, m - m0 + 1] .^ s)) for m in range_ms]\n",
    "    \n",
    "    data = DataFrame(X=log_ms, Y=log_expectations_train)\n",
    "    reg = lm(@formula(Y ~ X), data)\n",
    "    reg_dict_2[string(N)] = reg\n",
    "\n",
    "    display(reg)\n",
    "    \n",
    "    println()\n",
    "    println(\"R squared on the training set is: \", r2(reg))\n",
    "    println()\n",
    "    \n",
    "    # compute R^2 on the test set (see e.g. Wikipedia article), naively\n",
    "    SS_tot_test = sum((log_expectations_test .- mean(log_expectations_test)).^2)\n",
    "    SS_res_test = sum((log_expectations_test .- (coef(reg)[1] .+ coef(reg)[2] .* log_ms)).^2)\n",
    "    r2_test = 1 - SS_res_test/SS_tot_test\n",
    "    \n",
    "    println(\"R squared (naive) on the test set is: \", r2_test)\n",
    "    println()\n",
    "    \n",
    "    # compute out-of-sample OOS R^2, against predicted y_bar on the training set ! on the test set (see e.g. Wikipedia article), naively\n",
    "    SS_tot_train_test = sum((log_expectations_test .- mean(log_expectations_train)).^2)\n",
    "    r2_oos = 1 - SS_res_test/SS_tot_train_test\n",
    "    \n",
    "    println(\"R squared (out-of-sample) on the test set is: \", r2_oos)\n",
    "    println(\"\\n--------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    coeff_dict[string(N)] = Dict([(\"b0\", coef(reg)[1]), (\"b1\", coef(reg)[2]), \n",
    "            (\"r2_train\", r2(reg)), (\"r2_test\", r2_test), (\"r2_oos\", r2_oos)])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a6a1ce-bc9e-41de-afe3-587b0c4d97ba",
   "metadata": {},
   "source": [
    "## Miniconclusion\n",
    "\n",
    "Even with a train-test split of 60-40 or 80-20, the relation looks solidly linear. Note one thing though: our variable $x$ vector never changes, it remains constant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
