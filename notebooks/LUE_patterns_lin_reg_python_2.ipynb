{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5508bebc-6389-43f7-a19f-bd2f7c6f3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Author: Dan Betea                                                     #\n",
    "# (C) December 2022                                                     #\n",
    "# License: CC BY-SA 4.0                                                 #\n",
    "# License description: https://creativecommons.org/licenses/by-sa/4.0/  #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae910e7a-e2c7-4dfd-ba2c-a14edd76ca49",
   "metadata": {},
   "source": [
    "# LUE patterns 2: linear regression with naive train/test split\n",
    "\n",
    "Consider the Laguerre-$\\alpha$ Unitary Ensemble (LUE-$\\alpha$) distribution on ordered tuples of $N$ positive real numbers $(\\lambda_1 < \\dots < \\lambda_N).$ That is, consider the probability measure\n",
    "\n",
    "$$P(\\lambda_1, \\dots, \\lambda_N)d \\lambda_1 \\dots d \\lambda_N \\propto \\prod_{1 \\leq i < j \\leq N} (\\lambda_i - \\lambda_j)^2 \\prod_{1 \\leq i \\leq N} \\lambda_i^{\\alpha-1} e^{-\\lambda_i} d \\lambda_i$$\n",
    "\n",
    "where $\\alpha > 0$ (notice the somewhat less standard $\\alpha$ convention we use for LUE). See [this Wikipedia article](https://en.wikipedia.org/wiki/Complex_Wishart_distribution) for the motivation behind this distribution and how it comes about when studying covariance matrices (see in particular the Eigenvalues section).\n",
    "\n",
    "We do linear regression (in Python, using Scikit-learn) on \n",
    "\n",
    "$$(\\log i, \\log E \\lambda_i^s)$$\n",
    "\n",
    "for $i$ in a certain interval like $1, \\dots, 10; 1, \\dots, \\log N$ or more generally $m_0, \\dots, m_0 + M - 1$. Here $\\lambda_i$ the $i$-th lowest eigenvalue of the LUE-$\\alpha$ ensemble and $s$ is a real number, taken negative for convergence. \n",
    "\n",
    "We consider several cases:\n",
    "\n",
    "- $m_0 = 1, M = 15$, $\\alpha = 4, s = -2$, $N \\in \\{1000, 5000, 10000\\}$, and 1000 total samples\n",
    "\n",
    "and we additionally consider two cases for splitting the data into training and testing: \n",
    "\n",
    "- first we try 60%-40%, and\n",
    "- then we try 80%-20%.\n",
    "\n",
    "One we have variables $y_i^{train}$ and $y_i^{test}$, computing the $R^2$ coefficient (metric) becomes a matter of choice. We compute three such coefficients: on the training set only, on the test set only, and out-of-sample on the train and test set as follows:\n",
    "\n",
    "$$R^2_{train} = 1 - \\frac{\\sum_i (y_i^{train} - \\hat{y}_i^{train})^2}{\\sum_i (y_i^{train} - \\bar{y}^{train})^2}, \\qquad\n",
    "R^2_{test} = 1 - \\frac{\\sum_i (y_i^{test} - \\hat{y}_i^{test})^2}{\\sum_i (y_i^{test} - \\bar{y}^{test})^2}, \\qquad\n",
    "R^2_{oos} = 1 - \\frac{\\sum_i (y_i^{test} - \\hat{y}_i^{test})^2}{\\sum_i (y_i^{test} - \\bar{y}^{train})^2}$$\n",
    "\n",
    "and the reason for $R^2_{oos}$ is the following: we check the test set performance against the benchmark *non-prediction model* we see on the training/fitted set, which is the model which outputs the mean of the training set $\\bar{y}^{train}$ no matter the data.\n",
    "\n",
    "**Important remark:** The dependent variable $x = (\\log i)_{m_0 \\leq i \\leq m_0+M}$ is deterministic, so $\\hat{y}^{train} = \\hat{y}^{test}$ using our setup.\n",
    "\n",
    "**Some other remarks:**\n",
    "\n",
    "- array indexing starts at 0 by default in Python\n",
    "- the data files, containing iid samples $(\\lambda_1 < \\dots < \\lambda_N)$, are assumed to be in the same directory as the notebook, and be in the correct format\n",
    "- steps below can be automated; for this exploratory notebook they are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4f46422-58e1-4949-af3a-a63485847c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary packages\n",
    "import numpy as np                 # for linear algebra and loading from files\n",
    "from sklearn import linear_model   # for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a975d9-fe63-4362-b840-3d37f2d47a2d",
   "metadata": {},
   "source": [
    "First we set up the parameters independent of the data split: the values of $N$, $\\alpha$, $s$, $m_0, M$, and the $x$ vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4845d709-620e-4487-8c95-c0db8696e50c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = log i = \n",
      "[0.         0.69314718 1.09861229 1.38629436 1.60943791 1.79175947\n",
      " 1.94591015 2.07944154 2.19722458 2.30258509 2.39789527 2.48490665\n",
      " 2.56494936 2.63905733 2.7080502 ]\n"
     ]
    }
   ],
   "source": [
    "Ns = [1000, 5000, 10000]   # matrix sizes to be considered\n",
    "s = -2.0                   # point where to compute E \\lambda_i^s\n",
    "alpha_str = \"4.00\"\n",
    "alpha = float(alpha_str)   # = 4.0, Laguerre parameter alpha-1 \n",
    "m0 = 0                     # regression starts at \\lambda_{m_0+1}\n",
    "M = 15                     # regression ends with \\lambda_{m_0+M} \n",
    "range_ms = range(m0, m0+M) # index range for regression\n",
    "\n",
    "# building up dictionary of filenames (data is read from these files)\n",
    "filename_dict = {}\n",
    "for N in Ns:\n",
    "    filename = f\"LUE_N_{N}_alpha_{alpha_str}.txt\"\n",
    "    filename_dict[str(N)] = filename\n",
    "    \n",
    "# build the x variables for regression\n",
    "# x = log i for m0+1 <= i <= m0+M (+1 because of Python 0-indexed arrays) \n",
    "# reshape into column vectors needed\n",
    "log_ms = np.log(np.arange(m0+1, m0+M+1)).reshape(-1, 1)\n",
    "print(\"x = log i = \")\n",
    "print(log_ms.flatten())\n",
    "# print(\"\\n\"+73*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f8cf52-8890-49a2-ac4d-69c9376b3db3",
   "metadata": {},
   "source": [
    "### First case: 60-40 split\n",
    "\n",
    "Next we set up the train and test data, namely the $y$ vectors corresponding to train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1784cd40-55cc-49f6-8df4-a15994606d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N = 1000\n",
      "\n",
      "A_train = eigenvalue sample matrix (iid / line) = \n",
      "\n",
      "[[0.00914148 0.01421346 0.02985021 ... 0.42833932 0.45033342 0.52411581]\n",
      " [0.01065387 0.016576   0.02827056 ... 0.42273216 0.51435102 0.55804161]\n",
      " [0.00897166 0.01592701 0.0399726  ... 0.42665176 0.51484552 0.53870744]\n",
      " ...\n",
      " [0.0043258  0.01206655 0.03115058 ... 0.45655002 0.57092822 0.67062551]\n",
      " [0.00438205 0.03887275 0.05732078 ... 0.50094824 0.55490721 0.66749578]\n",
      " [0.0058209  0.01021484 0.035881   ... 0.46072031 0.55426236 0.60527526]]\n",
      "\n",
      "shape of A_train is: (1000, 15)\n",
      "\n",
      "log E \\lambda_i^s (train) = \n",
      "\n",
      "[10.60487598  8.10004521  6.73538035  5.75510236  4.9694163   4.32724725\n",
      "  3.77248462  3.28860453  2.84765712  2.45641598  2.11183603  1.7882116\n",
      "  1.47478295  1.201755    0.94275976]\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "N = 5000\n",
      "\n",
      "A_train = eigenvalue sample matrix (iid / line) = \n",
      "\n",
      "[[0.00114    0.00280814 0.00431431 ... 0.0926519  0.11608572 0.11824966]\n",
      " [0.00111132 0.00349919 0.0055273  ... 0.10164782 0.11596621 0.13266211]\n",
      " [0.00043231 0.00354626 0.00623526 ... 0.10506654 0.11092772 0.12206301]\n",
      " ...\n",
      " [0.00144574 0.00526298 0.01049865 ... 0.09467053 0.10503609 0.12177519]\n",
      " [0.00066836 0.00682101 0.01005465 ... 0.100075   0.13061468 0.13990584]\n",
      " [0.0026046  0.00520637 0.0079645  ... 0.08808057 0.100557   0.11083758]]\n",
      "\n",
      "shape of A_train is: (1000, 15)\n",
      "\n",
      "log E \\lambda_i^s (train) = \n",
      "\n",
      "[13.69455166 11.29388867  9.96698935  8.92929003  8.16342951  7.51796829\n",
      "  6.97376252  6.48424481  6.06317906  5.67491726  5.32283372  5.0050972\n",
      "  4.70035352  4.41509341  4.15448704]\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "N = 10000\n",
      "\n",
      "A_train = eigenvalue sample matrix (iid / line) = \n",
      "\n",
      "[[0.00033733 0.00259883 0.0047951  ... 0.0440713  0.05207287 0.06427409]\n",
      " [0.00077967 0.00120705 0.00350779 ... 0.04596401 0.05579818 0.06630394]\n",
      " [0.00040075 0.00085289 0.00203974 ... 0.04768492 0.05348894 0.06787949]\n",
      " ...\n",
      " [0.00100661 0.00165194 0.00482695 ... 0.05863087 0.06472924 0.06977831]\n",
      " [0.00095886 0.00224867 0.00521771 ... 0.04886442 0.05580353 0.0567986 ]\n",
      " [0.0017652  0.00242253 0.00349969 ... 0.04788582 0.05601745 0.0645648 ]]\n",
      "\n",
      "shape of A_train is: (1000, 15)\n",
      "\n",
      "log E \\lambda_i^s (train) = \n",
      "\n",
      "[15.21639962 12.69603267 11.33922138 10.32955918  9.55388277  8.93154929\n",
      "  8.35822421  7.8634264   7.44821947  7.05668318  6.69737211  6.37099209\n",
      "  6.07400957  5.7990274   5.53662213]\n",
      "\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# build up a dictionary of the data for each N and\n",
    "# load up the corresponding matrix A, only M columns: m0, ..., m0+M-1 and\n",
    "# build the y variables for regression into a dictionary, one vector per N\n",
    "# (same reshape needed as above)\n",
    "log_expectations_train_dict = {}\n",
    "log_expectations_test_dict = {}\n",
    "for N in Ns:\n",
    "    A = np.loadtxt(filename_dict[str(N)], usecols=range_ms)\n",
    "    A = A[:1000, :]              # take only 1000 samples, for consistency\n",
    "    A_train = A[:600, :]         # 80 % for training\n",
    "    A_test = A[600:, :]          # 20 % test\n",
    "    print()\n",
    "    print(f\"N = {N}\\n\")\n",
    "    print(\"A_train = eigenvalue sample matrix (iid / line) = \\n\")\n",
    "    print(A_train)\n",
    "    print()\n",
    "    print(f\"shape of A_train is: {A.shape}\\n\")\n",
    "    # below: axis = 0 means we sum over rows, i.e. take the average of each column (iid eigenval sample)\n",
    "    # build the y variable, for training\n",
    "    log_expectations_train = np.log(np.mean(A_train**s, axis=0)).reshape(-1, 1) \n",
    "    log_expectations_train_dict[str(N)] = log_expectations_train\n",
    "    # build the y variable, for testing\n",
    "    log_expectations_test = np.log(np.mean(A_test**s, axis=0)).reshape(-1, 1) \n",
    "    log_expectations_test_dict[str(N)] = log_expectations_test\n",
    "    print(\"log E \\lambda_i^s (train) = \\n\")\n",
    "    print(log_expectations_train.flatten())\n",
    "    print(\"\\n\"+73*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5bfa35-a438-47a5-a413-06aa81576c2b",
   "metadata": {},
   "source": [
    "#### Training (followed by testing)\n",
    "\n",
    "The actual regression goes down below. First the training phase to get the regression line.\n",
    "\n",
    "**Note:** The actual $R^2$ coefficient displayed below is obviously computed on the training set, and expected to be big. It is still nevertheless *quite high*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77b4663a-7d2b-48b4-a277-a7e0c3d9e20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " N = 1000\n",
      "\n",
      " slope:             -3.5565342748568156\n",
      " intercept:         10.640085997010413\n",
      " R squared (train):  0.9997252840928547\n",
      " R squared (test):   0.9996010133806793\n",
      " R squared (oos):    0.9996010244247339\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      " N = 5000\n",
      "\n",
      " slope:             -3.524984873534735\n",
      " intercept:         13.780306377117752\n",
      " R squared (train):  0.9996525781321135\n",
      " R squared (test):   0.9995437008969466\n",
      " R squared (oos):    0.9995437012847317\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      " N = 10000\n",
      "\n",
      " slope:             -3.560837784875215\n",
      " intercept:         15.241066745513617\n",
      " R squared (train):  0.9997641013695571\n",
      " R squared (test):   0.9986867445554635\n",
      " R squared (oos):    0.9986867990895595\n",
      "\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# doing the actual regression\n",
    "reg_dict = {} # dictionary to hold the regressors\n",
    "# below: dictionary to hold the coefficients of the regression in the form\n",
    "# str(N): {\"b0\": ..., \"b1\": ..., \"r2\": ...}\n",
    "# with b0 = intercept, b1 = slope, r2 = R squared\n",
    "coeff_dict = {str(N) : {} for N in Ns} \n",
    "for N in Ns:\n",
    "    print(f\"\\n N = {N}\\n\")\n",
    "    log_expectations_train = log_expectations_train_dict[str(N)]\n",
    "    log_expectations_test = log_expectations_test_dict[str(N)]\n",
    "    reg_dict[str(N)] = linear_model.LinearRegression()\n",
    "    reg_dict[str(N)].fit(log_ms, log_expectations_train) # linear fit\n",
    "    reg = reg_dict[str(N)]\n",
    "    \n",
    "    # computing R^2 for train and test (naive)\n",
    "    r2_train = reg.score(log_ms, log_expectations_train)\n",
    "    r2_test = reg.score(log_ms, log_expectations_test)\n",
    "    # computing the R^2 out-of-sample coefficient\n",
    "    SS_tot_train_test = np.sum((log_expectations_test - np.mean(log_expectations_train))**2)\n",
    "    SS_res_test = np.sum((log_expectations_test - (reg.intercept_.item() + reg.coef_.item() * log_ms))**2)\n",
    "    r2_oos = 1 - SS_res_test/SS_tot_train_test\n",
    "     \n",
    "    print(f\" slope:             {reg.coef_.item()}\")\n",
    "    print(f\" intercept:         {reg.intercept_.item()}\")\n",
    "    print(f\" R squared (train):  {r2_train}\")\n",
    "    print(f\" R squared (test):   {r2_test}\")\n",
    "    print(f\" R squared (oos):    {r2_oos}\")\n",
    "    coeff_dict[str(N)][\"b0\"] = reg.coef_.item()\n",
    "    coeff_dict[str(N)][\"b1\"] = reg.intercept_.item()\n",
    "    coeff_dict[str(N)][\"r2_train\"] = r2_train\n",
    "    coeff_dict[str(N)][\"r2_test\"] = r2_test\n",
    "    coeff_dict[str(N)][\"r2_oos\"] = r2_oos\n",
    "    print(\"\\n\"+73*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d915712d-3ba3-4ee9-9acc-ae2543eb6e04",
   "metadata": {},
   "source": [
    "### Second case: 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d29fd4e-b74d-4a0e-857e-7a5e2f339c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N = 1000\n",
      "\n",
      "A_train = eigenvalue sample matrix (iid / line) = \n",
      "\n",
      "[[0.00405837 0.01576241 0.03433577 ... 0.51706077 0.57475493 0.6476627 ]\n",
      " [0.00648518 0.01320577 0.03985386 ... 0.49188188 0.5384115  0.63590799]\n",
      " [0.0109845  0.03155448 0.04758467 ... 0.47631312 0.51299513 0.60330846]\n",
      " ...\n",
      " [0.0104201  0.01963166 0.03846435 ... 0.43218855 0.51115892 0.55671524]\n",
      " [0.00582622 0.00984355 0.03220857 ... 0.43013025 0.51010797 0.61515227]\n",
      " [0.01404306 0.02702059 0.04454848 ... 0.43967195 0.58155765 0.72504212]]\n",
      "\n",
      "shape of A_train is: (1000, 15)\n",
      "\n",
      "log E \\lambda_i^s (train) = \n",
      "\n",
      "[10.6138792   8.09176491  6.72208319  5.73931204  4.96804947  4.33075151\n",
      "  3.76904102  3.28213565  2.84679669  2.45226633  2.10730741  1.78289361\n",
      "  1.47532656  1.19744723  0.94050505]\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "N = 5000\n",
      "\n",
      "A_train = eigenvalue sample matrix (iid / line) = \n",
      "\n",
      "[[0.00250476 0.00369888 0.00814249 ... 0.09167625 0.11054006 0.12709592]\n",
      " [0.00116709 0.00259737 0.00807019 ... 0.09373403 0.10646009 0.1226777 ]\n",
      " [0.00039302 0.00395358 0.00529761 ... 0.10341038 0.11384548 0.13902808]\n",
      " ...\n",
      " [0.00045616 0.00319769 0.0064798  ... 0.09133178 0.11651967 0.12998098]\n",
      " [0.00226186 0.0043195  0.00913852 ... 0.1156643  0.13076585 0.14402008]\n",
      " [0.00060436 0.00350177 0.00719529 ... 0.08875277 0.10719097 0.10839797]]\n",
      "\n",
      "shape of A_train is: (1000, 15)\n",
      "\n",
      "log E \\lambda_i^s (train) = \n",
      "\n",
      "[13.66246821 11.30848859  9.92924621  8.93900948  8.1749269   7.52319692\n",
      "  6.96911512  6.48355978  6.06256024  5.67470078  5.320529    5.00174115\n",
      "  4.6969904   4.41269979  4.15538788]\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "N = 10000\n",
      "\n",
      "A_train = eigenvalue sample matrix (iid / line) = \n",
      "\n",
      "[[0.00122284 0.00334067 0.0046821  ... 0.05742837 0.05956978 0.07414231]\n",
      " [0.00075535 0.00152887 0.00260423 ... 0.05078005 0.05502777 0.06151355]\n",
      " [0.00068556 0.00139475 0.00317724 ... 0.04797801 0.05471206 0.05984219]\n",
      " ...\n",
      " [0.00138265 0.00230368 0.00389185 ... 0.04685326 0.05335075 0.06536883]\n",
      " [0.00107706 0.00250075 0.00369636 ... 0.04957134 0.0542272  0.06526369]\n",
      " [0.0012579  0.00231975 0.00431389 ... 0.04635724 0.05579029 0.06257022]]\n",
      "\n",
      "shape of A_train is: (1000, 15)\n",
      "\n",
      "log E \\lambda_i^s (train) = \n",
      "\n",
      "[15.08636083 12.66940511 11.34628246 10.33403811  9.54937016  8.91703858\n",
      "  8.35667485  7.87483933  7.44832389  7.06140227  6.70579201  6.37958384\n",
      "  6.0840979   5.80731564  5.54662223]\n",
      "\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# build up a dictionary of the data for each N and\n",
    "# load up the corresponding matrix A, only M columns: m0, ..., m0+M-1 and\n",
    "# build the y variables for regression into a dictionary, one vector per N\n",
    "# (same reshape needed as above)\n",
    "log_expectations_train_dict_2 = {}\n",
    "log_expectations_test_dict_2 = {}\n",
    "for N in Ns:\n",
    "    A = np.loadtxt(filename_dict[str(N)], usecols=range_ms)\n",
    "    A = A[:1000, :]              # take only 1000 samples, for consistency\n",
    "    A_train = A[200:, :]         # 80 % for training\n",
    "    A_test = A[:200, :]          # 20 % test\n",
    "    print()\n",
    "    print(f\"N = {N}\\n\")\n",
    "    print(\"A_train = eigenvalue sample matrix (iid / line) = \\n\")\n",
    "    print(A_train)\n",
    "    print()\n",
    "    print(f\"shape of A_train is: {A.shape}\\n\")\n",
    "    # below: axis = 0 means we sum over rows, i.e. take the average of each column (iid eigenval sample)\n",
    "    # build the y variable, for training\n",
    "    log_expectations_train = np.log(np.mean(A_train**s, axis=0)).reshape(-1, 1) \n",
    "    log_expectations_train_dict_2[str(N)] = log_expectations_train\n",
    "    # build the y variable, for testing\n",
    "    log_expectations_test = np.log(np.mean(A_test**s, axis=0)).reshape(-1, 1) \n",
    "    log_expectations_test_dict_2[str(N)] = log_expectations_test\n",
    "    print(\"log E \\lambda_i^s (train) = \\n\")\n",
    "    print(log_expectations_train.flatten())\n",
    "    print(\"\\n\"+73*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ab6482-73d6-45c8-b3a3-fa4c2a8ba56f",
   "metadata": {},
   "source": [
    "#### Training (followed by testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e671fc6a-a041-4549-a3d7-6315476a7f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " N = 1000\n",
      "\n",
      " slope:             -3.556985804860991\n",
      " intercept:         10.637124810312272\n",
      " R squared (train):  0.9997278908057867\n",
      " R squared (test):   0.9993046075393165\n",
      " R squared (oos):    0.9993046167559675\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      " N = 5000\n",
      "\n",
      " slope:             -3.5184585909075796\n",
      " intercept:         13.765136769385732\n",
      " R squared (train):  0.9996063762935679\n",
      " R squared (test):   0.9996495122867071\n",
      " R squared (oos):    0.9996495175602309\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      " N = 10000\n",
      "\n",
      " slope:             -3.525326475682311\n",
      " intercept:         15.168079151939518\n",
      " R squared (train):  0.9996381045595585\n",
      " R squared (test):   0.9996484001983278\n",
      " R squared (oos):    0.9996484010579837\n",
      "\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# doing the actual regression\n",
    "reg_dict_2 = {} # dictionary to hold the regressors\n",
    "# below: dictionary to hold the coefficients of the regression in the form\n",
    "# str(N): {\"b0\": ..., \"b1\": ..., \"r2\": ...}\n",
    "# with b0 = intercept, b1 = slope, r2 = R squared\n",
    "coeff_dict_2 = {str(N) : {} for N in Ns}\n",
    "for N in Ns:\n",
    "    print(f\"\\n N = {N}\\n\")\n",
    "    log_expectations_train = log_expectations_train_dict_2[str(N)]\n",
    "    log_expectations_test = log_expectations_test_dict_2[str(N)]\n",
    "    reg_dict_2[str(N)] = linear_model.LinearRegression()\n",
    "    reg_dict_2[str(N)].fit(log_ms, log_expectations_train) # linear fit\n",
    "    reg = reg_dict_2[str(N)]\n",
    "    \n",
    "    # computing R^2 for train and test (naive)\n",
    "    r2_train = reg.score(log_ms, log_expectations_train)\n",
    "    r2_test = reg.score(log_ms, log_expectations_test)\n",
    "    # computing the R^2 out-of-sample coefficient\n",
    "    SS_tot_train_test = np.sum((log_expectations_test - np.mean(log_expectations_train))**2)\n",
    "    SS_res_test = np.sum((log_expectations_test - (reg.intercept_.item() + reg.coef_.item() * log_ms))**2)\n",
    "    r2_oos = 1 - SS_res_test/SS_tot_train_test\n",
    "     \n",
    "    print(f\" slope:             {reg.coef_.item()}\")\n",
    "    print(f\" intercept:         {reg.intercept_.item()}\")\n",
    "    print(f\" R squared (train):  {r2_train}\")\n",
    "    print(f\" R squared (test):   {r2_test}\")\n",
    "    print(f\" R squared (oos):    {r2_oos}\")\n",
    "    coeff_dict[str(N)][\"b0\"] = reg.coef_.item()\n",
    "    coeff_dict[str(N)][\"b1\"] = reg.intercept_.item()\n",
    "    coeff_dict[str(N)][\"r2_train\"] = r2_train\n",
    "    coeff_dict[str(N)][\"r2_test\"] = r2_test\n",
    "    coeff_dict[str(N)][\"r2_oos\"] = r2_oos\n",
    "    print(\"\\n\"+73*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e68b7-95f6-438b-b0e3-2bb16c7757cd",
   "metadata": {},
   "source": [
    "## Miniconclusion\n",
    "\n",
    "Even with a train-test split of 60-40 or 80-20, the relation looks solidly linear. Note one thing though: our variable $x$ vector never changes, it remains constant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
